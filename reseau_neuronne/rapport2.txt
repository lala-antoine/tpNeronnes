Rapport TP : Du Perceptron Simple aux Réseaux Multicouches et Rétropropagation
Introduction
Le perceptron simple, que nous avons étudié précédemment, est un modèle de base des réseaux de neurones, capable de résoudre des problèmes linéairement séparables. Cependant, il présente des limites majeures, notamment son incapacité à traiter des problèmes non-linéaires comme la fonction XOR. Pour dépasser ces limites, les réseaux multicouches (Multi-Layer Perceptron - MLP) et l’algorithme de rétropropagation ont été développés. Ce TP a pour objectif de comprendre ces avancées fondamentales, d’implémenter un réseau multicouches, et d’analyser ses performances sur des problèmes complexes.

Partie 1 : Limites du Perceptron Simple et Introduction aux Réseaux Multicouches
1.1 Limites du Perceptron Simple
Le perceptron simple ne peut modéliser que des frontières linéaires. Le célèbre exemple de la fonction XOR (qui vaut 1 si une seule entrée est active, 0 sinon) est un problème non-linéairement séparables et échappe donc à un perceptron simple. Cette limitation fut formalisée en 1969 par Minsky et Papert, ce qui ralentit considérablement la recherche sur les réseaux de neurones pendant une période appelée « l’hiver de l’IA ».

1.2 Architecture Multicouches
La solution consiste à empiler plusieurs couches de neurones : une couche d'entrée, une ou plusieurs couches cachées, puis une couche de sortie. Chaque couche cachée apprend à transformer les données en une représentation plus exploitable, ce qui permet au réseau de modéliser des relations non linéaires complexes.

Couche d'entrée : reçoit les données brutes.

Couches cachées : transforment progressivement les données via des fonctions d’activation non linéaires.

Couche de sortie : fournit la prédiction finale.

Cette architecture permet de "projeter" les données dans un espace latent où elles deviennent linéairement séparables.

1.3 Théorème d'Approximation Universelle
Un réseau multicouches avec une couche cachée suffisamment grande peut approximer n’importe quelle fonction continue à une précision arbitraire. Cela justifie le pouvoir expressif des MLP.
Il ne garantit pas pour autant que l’on puisse apprendre ces poids facilement, mais ouvre la voie à une modélisation flexible.

Le réseau est composé de plusieurs couches chaînées. La propagation avant transmet les données couche par couche, de l’entrée à la sortie.

Partie 3 : Rétropropagation (Backpropagation)
3.1 Principe
La rétropropagation est une application de la règle de la chaîne en calcul différentiel. Elle permet de calculer efficacement les gradients de la fonction de coût par rapport à tous les paramètres (poids et biais) du réseau, couche par couche, depuis la sortie vers l'entrée.

3.2 Calcul des Gradients
Pour chaque couche, on calcule :

Erreur de la couche de sortie (différence entre sortie réelle et prédite)

Delta (erreur locale) : produit de l’erreur et de la dérivée de la fonction d’activation

Gradients : dérivées du coût par rapport aux poids et biais

Propagation de l’erreur vers la couche précédente

Les poids et biais sont ensuite mis à jour par descente de gradient, selon un taux d’apprentissage donné.

3.3 Exemple Numérique
Un réseau simple 2-2-1 est utilisé pour illustrer les calculs de propagation avant et arrière sur des entrées et cibles précises.

Partie 4 : Application et Résultats
4.1 Résolution du Problème XOR
Le réseau multicouches est testé sur le problème XOR, qu’un perceptron simple ne peut résoudre.

Plusieurs architectures sont essayées : [2, 2, 1], [2, 3, 1], [2, 4, 1], etc.

Le réseau apprend à classifier correctement les sorties, montrant que l’ajout de couches cachées et la rétropropagation permettent de traiter des problèmes non linéaires.

4.2 Tests sur Datasets Synthétiques et Réels
Des jeux de données plus complexes sont utilisés pour tester l’impact de la profondeur, largeur et fonctions d’activation du réseau sur la qualité d’apprentissage.

4.3 Courbes d’Apprentissage
Les courbes de perte (loss) et d’accuracy (précision) pendant l’entraînement montrent :

La convergence progressive du modèle

Les risques de sur-apprentissage (overfitting) si le modèle est trop complexe ou l’entraînement trop long

L’utilité de la validation pour détecter le sur-apprentissage

Partie 5 : Discussion
5.1 Avantages et Inconvénients des Réseaux Multicouches
Avantages : Puissance d’expression élevée, capacité à modéliser des fonctions complexes, adaptabilité à divers problèmes.

Inconvénients : Temps de calcul important, nécessité d’un réglage fin des hyperparamètres, risque de sur-apprentissage, besoin de beaucoup de données.

5.2 Sur-Apprentissage et Sous-Apprentissage
Sous-apprentissage (underfitting) : modèle trop simple pour capturer la complexité des données.

Sur-apprentissage (overfitting) : modèle trop complexe qui apprend le bruit et ne généralise pas bien.

5.3 Stratégies de Régularisation
Pour lutter contre le sur-apprentissage, on utilise :

Early stopping (arrêt anticipé)

Régularisation L2 (pénalisation des poids)

Dropout (désactivation aléatoire de neurones lors de l’entraînement)

Augmentation des données

Conclusion et Perspectives
Le passage du perceptron simple aux réseaux multicouches marque une étape décisive dans l’apprentissage automatique. Grâce à l’algorithme de rétropropagation, ces réseaux peuvent apprendre à modéliser des fonctions non linéaires complexes, ouvrant la voie à des applications variées en reconnaissance d’image, traitement du langage, et plus encore. Le TP montre que l’architecture du réseau, la fonction d’activation, et les techniques d’optimisation sont des facteurs clés à maîtriser pour obtenir de bonnes performances.

Les perspectives incluent l’exploration de réseaux plus profonds (deep learning), d’autres fonctions d’activation modernes (ReLU, Leaky ReLU), et des méthodes avancées de régularisation et optimisation.

Visualisations Proposées
Surfaces de décision 2D sur le problème XOR

Courbes de perte et précision en fonction des époques

Visualisation des poids appris dans les couches cachées

Comparaison des performances selon l’architecture (nombre de couches/neuronnes)

Analyse du sur-apprentissage via courbes d’erreur entraînement vs validation